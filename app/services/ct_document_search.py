from app.elasticsearch.client import get_es_client
from app.services.embedding_service import embedding_service
from typing import Dict, Any, List
import json

es = get_es_client()

def full_text_search_ct_documents(index_name: str, search_text: str):
    """ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
    query = {
        "query": {
            "multi_match": {
                "query": search_text,
                "fields": [
                    "product_name^3",
                    "search_text^2", 
                    "lab_info^1.5",
                    "special_notes.*^1"
                ],
                "type": "best_fields",
                "fuzziness": "AUTO"
            }
        },
        "highlight": {
            "fields": {
                "product_name": {},
                "search_text": {
                    "fragment_size": 150,
                    "number_of_fragments": 3
                },
                "lab_info": {
                    "fragment_size": 100,
                    "number_of_fragments": 2
                }
            }
        }
    }
    
    print(f"[ì¿¼ë¦¬ ë¡œê·¸][ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰]\n{json.dumps(query, ensure_ascii=False, indent=2)}")
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def advanced_search_ct_documents(index_name: str, search_params: Dict[str, Any]):
    """ê³ ê¸‰ ê²€ìƒ‰ (ë‹¤ì¤‘ ì¡°ê±´)"""
    must_conditions = []
    filter_conditions = []
    
    # ì œí’ˆëª… ê²€ìƒ‰
    if search_params.get('product_name'):
        must_conditions.append({
            "match": {
                "product_name": {
                    "query": search_params['product_name'],
                    "operator": "or"
                }
            }
        })
    
    # ê³ ê°ì‚¬ í•„í„°
    if search_params.get('customer'):
        filter_conditions.append({
            "term": {"customer": search_params['customer']}
        })
    
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²€ìƒ‰
    if search_params.get('test_code'):
        must_conditions.append({
            "nested": {
                "path": "experiment_info",
                "query": {
                    "term": {"experiment_info.code": search_params['test_code']}
                }
            }
        })
    
    # ì¬ì§ˆ ê²€ìƒ‰
    if search_params.get('material'):
        must_conditions.append({
            "nested": {
                "path": "packing_info",
                "query": {
                    "match": {"packing_info.material": search_params['material']}
                }
            }
        })
    
    # ë‚ ì§œ ë²”ìœ„ í•„í„°
    if search_params.get('start_date') or search_params.get('end_date'):
        date_range = {}
        if search_params.get('start_date'):
            date_range['gte'] = search_params['start_date']
        if search_params.get('end_date'):
            date_range['lte'] = search_params['end_date']
        
        filter_conditions.append({
            "range": {"test_date": date_range}
        })
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰
    if search_params.get('search_text'):
        must_conditions.append({
            "multi_match": {
                "query": search_params['search_text'],
                "fields": ["search_text^2", "lab_info", "special_notes.*"],
                "type": "best_fields"
            }
        })
    
    query = {
        "query": {
            "bool": {
                "must": must_conditions,
                "filter": filter_conditions
            }
        },
        "highlight": {
            "fields": {
                "product_name": {},
                "search_text": {
                    "fragment_size": 150,
                    "number_of_fragments": 3
                }
            }
        }
    }
    
    print(f"[ì¿¼ë¦¬ ë¡œê·¸][ê³ ê¸‰ ê²€ìƒ‰]\n{json.dumps(query, ensure_ascii=False, indent=2)}")
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"ê³ ê¸‰ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def get_ct_document_statistics(index_name: str):
    """CT ë¬¸ì„œ í†µê³„ ì •ë³´ ì¡°íšŒ"""
    query = {
        "size": 0,
        "aggs": {
            "customer_count": {
                "terms": {"field": "customer", "size": 20}
            },
            "test_count_distribution": {
                "terms": {"field": "test_count"}
            },
            "date_distribution": {
                "date_histogram": {
                    "field": "test_date",
                    "calendar_interval": "month"
                }
            },
            "material_distribution": {
                "nested": {
                    "path": "packing_info"
                },
                "aggs": {
                    "materials": {
                        "terms": {"field": "packing_info.material"}
                    }
                }
            }
        }
    }
    
    print(f"[ì¿¼ë¦¬ ë¡œê·¸][í†µê³„ ì¡°íšŒ]\n{json.dumps(query, ensure_ascii=False, indent=2)}")
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return None

def print_ct_search_results(response, search_type: str):
    """CT ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥"""
    if not response:
        print(f"{search_type} ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return
    
    print(f"\n=== {search_type} ê²€ìƒ‰ ê²°ê³¼ ===")
    print(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {response['hits']['total']['value']}ê°œ")
    
    for hit in response['hits']['hits']:
        source = hit['_source']
        score = hit['_score']
        print(f"\në¬¸ì„œ ID: {hit['_id']} (ì ìˆ˜: {score:.2f})")
        print(f"ì œí’ˆëª…: {source.get('product_name', 'N/A')}")
        print(f"ê³ ê°ì‚¬: {source.get('customer', 'N/A')}")
        print(f"ê°œë°œì: {source.get('developer', 'N/A')}")
        print(f"í…ŒìŠ¤íŠ¸ ë‚ ì§œ: {source.get('test_date', 'N/A')}")
        print(f"ì‹¤í—˜ì‹¤ ID: {source.get('lab_id', 'N/A')}")
        print(f"í¬ì¥ ì •ë³´: {source.get('packing_info', 'N/A')}")
        
        # í•˜ì´ë¼ì´íŠ¸ ê²°ê³¼ ì¶œë ¥
        if 'highlight' in hit:
            print("ğŸ” í•˜ì´ë¼ì´íŠ¸ëœ ë§¤ì¹­ ë¶€ë¶„:")
            for field, highlights in hit['highlight'].items():
                print(f"  ğŸ“ {field}: {' ... '.join(highlights)}")
        
        print("-" * 80)

def print_ct_statistics(response):
    """CT ë¬¸ì„œ í†µê³„ ê²°ê³¼ ì¶œë ¥"""
    if not response:
        print("í†µê³„ ê²°ê³¼ ì—†ìŒ")
        return
    
    print(f"\n=== CT ë¬¸ì„œ í†µê³„ ===")
    
    aggs = response['aggregations']
    
    if 'customer_count' in aggs:
        print("\nê³ ê°ì‚¬ë³„ ë¬¸ì„œ ìˆ˜:")
        for bucket in aggs['customer_count']['buckets']:
            print(f"  {bucket['key']}: {bucket['doc_count']}ê°œ")
    
    if 'test_count_distribution' in aggs:
        print("\ní…ŒìŠ¤íŠ¸ ì°¨ìˆ˜ë³„ ë¶„í¬:")
        for bucket in aggs['test_count_distribution']['buckets']:
            print(f"  {bucket['key']}: {bucket['doc_count']}ê°œ")
    
    if 'date_distribution' in aggs:
        print("\nì›”ë³„ ë¬¸ì„œ ë¶„í¬:")
        for bucket in aggs['date_distribution']['buckets']:
            print(f"  {bucket['key_as_string']}: {bucket['doc_count']}ê°œ")

def search_ct_documents_by_packing_info(index_name: str, packing_type: str, material: str, spec: str = None, company: str = None):
    """í¬ì¥ ì •ë³´(íƒ€ì…, ì¬ì§ˆ, ì„¸ë¶€ì‚¬ì–‘, ì—…ì²´)ë¡œ CT ë¬¸ì„œ ê²€ìƒ‰ (íƒ€ì…, ì¬ì§ˆ í•„ìˆ˜, ì„¸ë¶€ì‚¬ì–‘/ì—…ì²´ ì„ íƒ)"""
    nested_query = {
        "bool": {
            "must": [
                {"term": {"packing_info.type": packing_type}},
                {"term": {"packing_info.material": material}}
            ]
        }
    }
    # ì„¸ë¶€ì‚¬ì–‘(spec) ì¡°ê±´ ì¶”ê°€ (ìˆì„ ë•Œë§Œ)
    if spec:
        nested_query["bool"]["must"].append({
            "match": {"packing_info.spec": spec}
        })
    # í¬ì¥ì¬ì—…ì²´(company) ì¡°ê±´ ì¶”ê°€ (ìˆì„ ë•Œë§Œ)
    if company:
        nested_query["bool"]["must"].append({
            "match": {"packing_info.company": company}
        })

    query = {
        "query": {
            "nested": {
                "path": "packing_info",
                "query": nested_query
            }
        }
    }
    print(f"[ì¿¼ë¦¬ ë¡œê·¸][í¬ì¥ ì •ë³´ ê²€ìƒ‰]\n{json.dumps(query, ensure_ascii=False, indent=2)}")
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"í¬ì¥ ì •ë³´ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None
    
def semantic_search_special_notes(index_name: str, query_text: str, threshold: float = 0.7, top_k: int = 10):
    """special_notesì˜ ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ (Elasticsearch dense_vector ì‚¬ìš©)"""
    try:
        # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ìƒì„±
        query_embedding = embedding_service.get_embedding(query_text)
        if not query_embedding:
            print("ì¿¼ë¦¬ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            return None
        
        print(f"ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ ì‹œì‘: '{query_text}' (ì„ê³„ê°’: {threshold})")
        
        # Elasticsearchì˜ dense_vector ê²€ìƒ‰ ì¿¼ë¦¬
        query = {
            "query": {
                "nested": {
                    "path": "special_notes",
                    "query": {
                        "script_score": {
                            "query": {
                                "exists": {
                                    "field": "special_notes.embedding"
                                }
                            },
                            "script": {
                                "source": "cosineSimilarity(params.query_vector, 'special_notes.embedding') + 1.0",
                                "params": {
                                    "query_vector": query_embedding
                                }
                            }
                        }
                    },
                    "inner_hits": {
                        "size": top_k,
                        "_source": ["special_notes.key", "special_notes.value"]
                    }
                }
            },
            "size": top_k
        }
        
        print(f"[ì¿¼ë¦¬ ë¡œê·¸][ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰]\n{json.dumps(query, ensure_ascii=False, indent=2)}")
        
        response = es.search(index=index_name, body=query)
        
        # ê²°ê³¼ ì²˜ë¦¬
        formatted_results = []
        for hit in response['hits']['hits']:
            source = hit['_source']
            score = hit['_score']
            
            # inner_hitsì—ì„œ special_notes ê²°ê³¼ ì¶”ì¶œ
            if 'inner_hits' in hit and 'special_notes' in hit['inner_hits']:
                for inner_hit in hit['inner_hits']['special_notes']['hits']['hits']:
                    inner_score = inner_hit['_score']
                    if inner_score >= threshold:
                        note = inner_hit['_source']
                        formatted_results.append({
                            '_id': hit['_id'],
                            '_score': inner_score,
                            '_source': source,
                            'highlight': {
                                'special_notes.value': [note['value']],
                                'special_notes.key': [note.get('key', '')]
                            }
                        })
        
        print(f"ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ ì™„ë£Œ: {len(formatted_results)}ê°œ ê²°ê³¼")
        
        return {
            'hits': {
                'total': {'value': len(formatted_results)},
                'hits': formatted_results
            }
        }
        
    except Exception as e:
        print(f"ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def hybrid_search_special_notes(index_name: str, query_text: str, 
                              text_boost: float = 1.0, semantic_boost: float = 2.0,
                              threshold: float = 0.7):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (í…ìŠ¤íŠ¸ ê²€ìƒ‰ + ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰)"""
    # í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰
    text_query = {
        "query": {
            "nested": {
                "path": "special_notes",
                "query": {
                    "match": {
                        "special_notes.value": {
                            "query": query_text,
                            "boost": text_boost
                        }
                    }
                }
            }
        },
        "highlight": {
            "fields": {
                "special_notes.value": {
                    "fragment_size": 200,
                    "number_of_fragments": 3
                },
                "special_notes.key": {
                    "fragment_size": 100,
                    "number_of_fragments": 2
                }
            }
        }
    }
    
    try:
        text_response = es.search(index=index_name, body=text_query)
        semantic_response = semantic_search_special_notes(index_name, query_text, threshold)
        
        # ê²°ê³¼ ë³‘í•©
        combined_hits = []
        
        # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for hit in text_response['hits']['hits']:
            hit['_score'] *= text_boost
            combined_hits.append(hit)
        
        # ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¤‘ë³µ ì œê±°)
        existing_ids = {hit['_id'] for hit in combined_hits}
        for hit in semantic_response['hits']['hits']:
            if hit['_id'] not in existing_ids:
                hit['_score'] *= semantic_boost
                combined_hits.append(hit)
        
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        combined_hits.sort(key=lambda x: x['_score'], reverse=True)
        
        return {
            'hits': {
                'total': {'value': len(combined_hits)},
                'hits': combined_hits
            }
        }
        
    except Exception as e:
        print(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def search_ct_documents_by_multiple_packing_sets(
        index_name: str, 
        packing_sets: list, 
        lab_id: str = None, 
        lab_info: str = None, 
        optimum_capacity: str = None, 
        special_note: str = None,
        test_date_start: str = None,
        test_date_end: str = None,
        use_semantic_search: bool = True,
        semantic_threshold: float = 0.7
    ):
    """
    ì—¬ëŸ¬ í¬ì¥ ì •ë³´ ì„¸íŠ¸ ì¤‘ í•˜ë‚˜ë¼ë„ ì¼ì¹˜í•˜ëŠ” CT ë¬¸ì„œ ê²€ìƒ‰ + lab_idë¡œë„ ê²€ìƒ‰
    packing_sets: [
        {"type": "íŠœë¸Œ", "material": "AS", "company": "ê±´ë™"},
        {"type": "ìš©ê¸°", "material": "PP"}
    ]
    lab_id: str (optional)
    use_semantic_search: bool - special_note ê²€ìƒ‰ ì‹œ ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
    """
    should_nested_queries = []
    for packing in packing_sets:
        must_conditions = []
        if packing.get("type"):
            must_conditions.append({"term": {"packing_info.type": packing["type"]}})
        if packing.get("material"):
            must_conditions.append({"term": {"packing_info.material": packing["material"]}})
        if packing.get("spec"):
            must_conditions.append({"match": {"packing_info.spec": packing["spec"]}})
        if packing.get("company"):
            must_conditions.append({"match": {"packing_info.company": packing["company"]}})
        if must_conditions:
            should_nested_queries.append({
                "nested": {
                    "path": "packing_info",
                    "query": {
                        "bool": {
                            "must": must_conditions
                        }
                    }
                }
            })
    
    # lab_id ì¡°ê±´ ì¶”ê°€
    must_queries = []
    if lab_id:
        must_queries.append({"term": {"lab_id": lab_id}})
    if lab_info:
        must_queries.append({"match": {"lab_info": lab_info}})
    if optimum_capacity:
        must_queries.append({"match": {"optimum_capacity": optimum_capacity}})

    # special_note ì¡°ê±´ ì¶”ê°€ (ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš© ì‹œ)
    if special_note and use_semantic_search:
        # ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ì„ ë³„ë„ë¡œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ í•„í„°ë§ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©
        semantic_results = semantic_search_special_notes(
            index_name, special_note, semantic_threshold
        )
        if semantic_results and semantic_results['hits']['hits']:
            # ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ ê²°ê³¼ì˜ ë¬¸ì„œ IDë“¤ì„ í•„í„°ë§ ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©
            doc_ids = [hit['_id'] for hit in semantic_results['hits']['hits']]
            must_queries.append({"terms": {"document_id": doc_ids}})
    elif special_note:
        # ê¸°ì¡´ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰
        must_queries.append({
            "nested": {
                "path": "special_notes",
                "query": {
                    "match_phrase": {
                        "special_notes.value": {
                            "query": special_note
                        }
                    }
                }
            }
        })

    # should ì¡°ê±´ì´ ìˆì„ ë•Œë§Œ minimum_should_match ì¶”ê°€
    bool_query = {
        "must": must_queries
    }
    if should_nested_queries:
        bool_query["should"] = should_nested_queries
        bool_query["minimum_should_match"] = 1
    
    if test_date_start:
        bool_query["must"].append({"range": {"test_date": {"gte": test_date_start}}})
    if test_date_end:
        bool_query["must"].append({"range": {"test_date": {"lte": test_date_end}}})
    
    query = {
        "query": {
            "bool": bool_query
        },
        "highlight": {
            "fields": {
                "special_notes.value": {
                    "fragment_size": 200,
                    "number_of_fragments": 3
                },
                "special_notes.key": {
                    "fragment_size": 100,
                    "number_of_fragments": 2
                }
            }
        }
    }
    
    print(f"[ì¿¼ë¦¬ ë¡œê·¸][ì—¬ëŸ¬ í¬ì¥ ì •ë³´ ì„¸íŠ¸ ê²€ìƒ‰]\n{json.dumps(query, ensure_ascii=False, indent=2)}")
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"ì—¬ëŸ¬ í¬ì¥ ì •ë³´ ì„¸íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

if __name__ == "__main__":
    index_name = "ct_documents"

    # 3. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\n3. ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ì œí’ˆëª… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # print("\n--- ì œí’ˆëª… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
    # result = search_ct_documents_by_product_name(index_name, "Lip")
    # print_ct_search_results(result, "Lip ì œí’ˆ ê²€ìƒ‰")
    
    # # ê³ ê°ì‚¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # print("\n--- ê³ ê°ì‚¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
    # result = search_ct_documents_by_customer(index_name, "Interstory")
    # print_ct_search_results(result, "Interstory ê³ ê°ì‚¬ ê²€ìƒ‰")
    
    # # ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # print("\n--- ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
    # result = full_text_search_ct_documents(index_name, "íŒí•‘ í…ŒìŠ¤íŠ¸")
    # print_ct_search_results(result, "íŒí•‘ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
    
    # # ì¬ì§ˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # print("\n--- ì¬ì§ˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
    # result = search_ct_documents_by_material(index_name, "AS")
    # print_ct_search_results(result, "AS ì¬ì§ˆ ê²€ìƒ‰")
    
    # # í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # print("\n--- í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
    # result = search_ct_documents_by_test_code(index_name, "TMM202")
    # print_ct_search_results(result, "TMM202 í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²€ìƒ‰")
    
    # ê³ ê¸‰ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # print("\n--- ê³ ê¸‰ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
    # search_params = {
    #     # "product_name": "Lip",
    #     # "customer": "Interstory",
    #     "search_text": "ë¬¼ê´‘"
    # }
    # result = advanced_search_ct_documents(index_name, search_params)
    # print_ct_search_results(result, "ê³ ê¸‰ ê²€ìƒ‰ (Lip + Interstory + íŒí•‘)")
    
    # # í†µê³„ ì¡°íšŒ í…ŒìŠ¤íŠ¸
    # print("\n--- í†µê³„ ì¡°íšŒ í…ŒìŠ¤íŠ¸ ---")
    # stats_result = get_ct_document_statistics(index_name)
    # print_ct_statistics(stats_result)
            

    input_data = {
        "packages": [
            {"type": "ìš©ê¸°", "material": "PET", "spec": "ì¸ì ì…˜ë¸Œë¡œìš°", "company": "ê±´ë™"},
            {"type": "ìº¡", "material": "PP", "spec": "ê³ ë¬´", "company": "ê±´ë™"}
        ],
        "lab_id": "LAB001",
        "lab_info": "ê±´ë™ ì‹¤í—˜ì‹¤",
        "optimum_capacity": "100ml"
    }

    result = search_ct_documents_by_multiple_packing_sets(index_name, input_data['packages'], input_data['lab_id'])
    # result
