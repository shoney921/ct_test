from elasticsearch import Elasticsearch
from dotenv import load_dotenv
import os
import json
import warnings
import urllib3
from typing import List, Dict, Any, Optional

# SSL ê²½ê³  ì–µì œ
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

ELASTICSEARCH_HOST = os.getenv("ELASTICSEARCH_HOST")
ELASTICSEARCH_USER = os.getenv("ELASTICSEARCH_USER")
ELASTICSEARCH_PASSWORD = os.getenv("ELASTICSEARCH_PASSWORD")

# ì—˜ë¼ìŠ¤í‹±ì„œì¹˜ í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ID/PW ì¸ì¦)
es = Elasticsearch(
    ELASTICSEARCH_HOST,
    basic_auth=(ELASTICSEARCH_USER, ELASTICSEARCH_PASSWORD),
    verify_certs=False,
    ssl_show_warn=False  # SSL ê²½ê³  ì–µì œ
)

def create_ct_document_index_with_mapping(index_name: str):
    """CT ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ì¸ë±ìŠ¤ ìƒì„± ë° ë§¤í•‘ ì„¤ì •"""
    mapping = {
        "mappings": {
            "properties": {
                # ê¸°ë³¸ ì •ë³´
                "file_name": {"type": "text", "analyzer": "standard"},
                "test_no": {"type": "keyword"},
                "product_name": {"type": "text", "analyzer": "standard"},
                "customer": {"type": "keyword"},
                "developer": {"type": "keyword"},
                "requester": {"type": "keyword"},
                "test_count": {"type": "keyword"},
                "test_quantity": {"type": "keyword"},
                "test_date": {"type": "date"},
                "expected_date": {"type": "date"},
                "writer": {"type": "keyword"},
                "reviewer": {"type": "keyword"},
                "approver": {"type": "keyword"},
                
                # ì‹¤í—˜ì‹¤ ì •ë³´
                "lab_id": {"type": "keyword"},
                "lab_info": {"type": "text", "analyzer": "standard"},
                
                # í¬ì¥ ì •ë³´ (nested object)
                "packing_info": {
                    "type": "nested",
                    "properties": {
                        "type": {"type": "keyword"},
                        "material": {"type": "keyword"},
                        "spec": {"type": "text", "analyzer": "standard"},
                        "company": {"type": "keyword"}
                    }
                },
                
                # ì‹¤í—˜ ì •ë³´ (nested object)
                "experiment_info": {
                    "type": "nested",
                    "properties": {
                        "code": {"type": "keyword"},
                        "item": {"type": "text", "analyzer": "standard"},
                        "period": {"type": "keyword"},
                        "check": {"type": "keyword"},
                        "standard": {"type": "text", "analyzer": "standard"},
                        "result": {"type": "text", "analyzer": "standard"}
                    }
                },
                
                # íŠ¹ë³„ ì°¸ê³ ì‚¬í•­ (nested object)
                "special_notes": {
                    "type": "nested",
                    "properties": {
                        "General": {"type": "text", "analyzer": "standard"},
                        "Package": {"type": "text", "analyzer": "standard"},
                        "Bulk": {"type": "text", "analyzer": "standard"},
                        "Productivity": {"type": "text", "analyzer": "standard"}
                    }
                },
                
                # ê²€ìƒ‰ì„ ìœ„í•œ í†µí•© í…ìŠ¤íŠ¸ í•„ë“œ
                "search_text": {"type": "text", "analyzer": "standard"},
                
                # ë©”íƒ€ë°ì´í„°
                "created_at": {"type": "date"},
                "updated_at": {"type": "date"},
                "tags": {"type": "keyword"},
                "status": {"type": "keyword"}
            }
        },
        "settings": {
            "analysis": {
                "analyzer": {
                    "korean_analyzer": {
                        "type": "custom",
                        "tokenizer": "standard",
                        "filter": ["lowercase", "stop"]
                    }
                }
            }
        }
    }
    
    try:
        # ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œ
        if es.indices.exists(index=index_name):
            es.indices.delete(index=index_name)
        
        # ìƒˆ ì¸ë±ìŠ¤ ìƒì„±
        es.indices.create(index=index_name, body=mapping)
        print(f"CT ë¬¸ì„œ ì¸ë±ìŠ¤ '{index_name}' ìƒì„± ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"ì¸ë±ìŠ¤ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        return False

def process_ct_document_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:
    """CT ë¬¸ì„œ ë°ì´í„°ë¥¼ ì—˜ë¼ìŠ¤í‹±ì„œì¹˜ì— ì í•©í•œ í˜•íƒœë¡œ ì „ì²˜ë¦¬"""
    processed_data = raw_data.copy()
    
    # ê²€ìƒ‰ì„ ìœ„í•œ í†µí•© í…ìŠ¤íŠ¸ í•„ë“œ ìƒì„±
    search_text_parts = []
    
    # ê¸°ë³¸ ì •ë³´ ì¶”ê°€
    if raw_data.get('product_name'):
        search_text_parts.append(raw_data['product_name'])
    if raw_data.get('customer'):
        search_text_parts.append(raw_data['customer'])
    if raw_data.get('lab_info'):
        search_text_parts.append(raw_data['lab_info'])
    
    # ì‹¤í—˜ ì •ë³´ì—ì„œ ê²€ìƒ‰ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    if raw_data.get('experiment_info'):
        for exp in raw_data['experiment_info']:
            if exp.get('item'):
                search_text_parts.append(exp['item'])
            if exp.get('standard'):
                search_text_parts.append(exp['standard'])
            if exp.get('result'):
                search_text_parts.append(exp['result'])
    
    # íŠ¹ë³„ ì°¸ê³ ì‚¬í•­ ì¶”ê°€
    if raw_data.get('special_notes'):
        for key, value in raw_data['special_notes'].items():
            if value:
                search_text_parts.append(value)
    
    # í¬ì¥ ì •ë³´ ì¶”ê°€
    if raw_data.get('packing_info'):
        for pack in raw_data['packing_info']:
            if pack.get('type'):
                search_text_parts.append(pack['type'])
            if pack.get('material'):
                search_text_parts.append(pack['material'])
            if pack.get('spec'):
                search_text_parts.append(pack['spec'])
    
    # í†µí•© ê²€ìƒ‰ í…ìŠ¤íŠ¸ ìƒì„±
    processed_data['search_text'] = ' '.join(search_text_parts)
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    from datetime import datetime
    processed_data['created_at'] = datetime.now().isoformat()
    processed_data['updated_at'] = datetime.now().isoformat()
    
    # íƒœê·¸ ìƒì„±
    tags = []
    if raw_data.get('customer'):
        tags.append(f"customer:{raw_data['customer']}")
    if raw_data.get('test_count'):
        tags.append(f"test_count:{raw_data['test_count']}")
    if raw_data.get('developer'):
        tags.append(f"developer:{raw_data['developer']}")
    
    processed_data['tags'] = tags
    
    return processed_data

def insert_ct_document(index_name: str, document_id: str, document_data: Dict[str, Any]):
    """CT ë¬¸ì„œë¥¼ ì¸ë±ìŠ¤ì— ì‚½ì…"""
    try:
        processed_data = process_ct_document_data(document_data)
        es.index(index=index_name, id=document_id, document=processed_data)
        print(f"ë¬¸ì„œ {document_id} ì‚½ì… ì™„ë£Œ: {document_data.get('product_name', 'Unknown')}")
        return True
    except Exception as e:
        print(f"ë¬¸ì„œ {document_id} ì‚½ì… ì˜¤ë¥˜: {str(e)}")
        return False

def bulk_insert_ct_documents(index_name: str, documents: List[Dict[str, Any]]):
    """ì—¬ëŸ¬ CT ë¬¸ì„œë¥¼ ì¼ê´„ ì‚½ì…"""
    success_count = 0
    error_count = 0
    
    for doc in documents:
        document_id = doc.get('test_no', f"doc_{success_count + error_count}")
        if insert_ct_document(index_name, document_id, doc):
            success_count += 1
        else:
            error_count += 1
    
    # ì¸ë±ìŠ¤ ìƒˆë¡œê³ ì¹¨
    es.indices.refresh(index=index_name)
    print(f"ì¼ê´„ ì‚½ì… ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {error_count}ê°œ")
    return success_count, error_count

def search_ct_documents_by_product_name(index_name: str, product_name: str):
    """ì œí’ˆëª…ìœ¼ë¡œ CT ë¬¸ì„œ ê²€ìƒ‰"""
    query = {
        "query": {
            "match": {
                "product_name": {
                    "query": product_name,
                    "operator": "or"
                }
            }
        }
    }
    
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"ì œí’ˆëª… ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def search_ct_documents_by_customer(index_name: str, customer: str):
    """ê³ ê°ì‚¬ë¡œ CT ë¬¸ì„œ ê²€ìƒ‰"""
    query = {
        "query": {
            "term": {
                "customer": customer
            }
        }
    }
    
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"ê³ ê°ì‚¬ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def search_ct_documents_by_test_code(index_name: str, test_code: str):
    """í…ŒìŠ¤íŠ¸ ì½”ë“œë¡œ CT ë¬¸ì„œ ê²€ìƒ‰"""
    query = {
        "query": {
            "nested": {
                "path": "experiment_info",
                "query": {
                    "term": {
                        "experiment_info.code": test_code
                    }
                }
            }
        }
    }
    
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def search_ct_documents_by_material(index_name: str, material: str):
    """í¬ì¥ ì¬ì§ˆë¡œ CT ë¬¸ì„œ ê²€ìƒ‰"""
    query = {
        "query": {
            "nested": {
                "path": "packing_info",
                "query": {
                    "match": {
                        "packing_info.material": material
                    }
                }
            }
        }
    }
    
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"ì¬ì§ˆ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def search_ct_documents_by_date_range(index_name: str, start_date: str, end_date: str):
    """ë‚ ì§œ ë²”ìœ„ë¡œ CT ë¬¸ì„œ ê²€ìƒ‰"""
    query = {
        "query": {
            "range": {
                "test_date": {
                    "gte": start_date,
                    "lte": end_date
                }
            }
        }
    }
    
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"ë‚ ì§œ ë²”ìœ„ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def full_text_search_ct_documents(index_name: str, search_text: str):
    """ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
    query = {
        "query": {
            "multi_match": {
                "query": search_text,
                "fields": [
                    "product_name^3",
                    "search_text^2", 
                    "lab_info^1.5",
                    "special_notes.*^1"
                ],
                "type": "best_fields",
                "fuzziness": "AUTO"
            }
        },
        "highlight": {
            "fields": {
                "product_name": {},
                "search_text": {
                    "fragment_size": 150,
                    "number_of_fragments": 3
                },
                "lab_info": {
                    "fragment_size": 100,
                    "number_of_fragments": 2
                }
            }
        }
    }
    
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def advanced_search_ct_documents(index_name: str, search_params: Dict[str, Any]):
    """ê³ ê¸‰ ê²€ìƒ‰ (ë‹¤ì¤‘ ì¡°ê±´)"""
    must_conditions = []
    filter_conditions = []
    
    # ì œí’ˆëª… ê²€ìƒ‰
    if search_params.get('product_name'):
        must_conditions.append({
            "match": {
                "product_name": {
                    "query": search_params['product_name'],
                    "operator": "or"
                }
            }
        })
    
    # ê³ ê°ì‚¬ í•„í„°
    if search_params.get('customer'):
        filter_conditions.append({
            "term": {"customer": search_params['customer']}
        })
    
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²€ìƒ‰
    if search_params.get('test_code'):
        must_conditions.append({
            "nested": {
                "path": "experiment_info",
                "query": {
                    "term": {"experiment_info.code": search_params['test_code']}
                }
            }
        })
    
    # ì¬ì§ˆ ê²€ìƒ‰
    if search_params.get('material'):
        must_conditions.append({
            "nested": {
                "path": "packing_info",
                "query": {
                    "match": {"packing_info.material": search_params['material']}
                }
            }
        })
    
    # ë‚ ì§œ ë²”ìœ„ í•„í„°
    if search_params.get('start_date') or search_params.get('end_date'):
        date_range = {}
        if search_params.get('start_date'):
            date_range['gte'] = search_params['start_date']
        if search_params.get('end_date'):
            date_range['lte'] = search_params['end_date']
        
        filter_conditions.append({
            "range": {"test_date": date_range}
        })
    
    # ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰
    if search_params.get('search_text'):
        must_conditions.append({
            "multi_match": {
                "query": search_params['search_text'],
                "fields": ["search_text^2", "lab_info", "special_notes.*"],
                "type": "best_fields"
            }
        })
    
    query = {
        "query": {
            "bool": {
                "must": must_conditions,
                "filter": filter_conditions
            }
        },
        "highlight": {
            "fields": {
                "product_name": {},
                "search_text": {
                    "fragment_size": 150,
                    "number_of_fragments": 3
                }
            }
        }
    }
    
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"ê³ ê¸‰ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return None

def get_ct_document_statistics(index_name: str):
    """CT ë¬¸ì„œ í†µê³„ ì •ë³´ ì¡°íšŒ"""
    query = {
        "size": 0,
        "aggs": {
            "customer_count": {
                "terms": {"field": "customer", "size": 20}
            },
            "test_count_distribution": {
                "terms": {"field": "test_count"}
            },
            "date_distribution": {
                "date_histogram": {
                    "field": "test_date",
                    "calendar_interval": "month"
                }
            },
            "material_distribution": {
                "nested": {
                    "path": "packing_info"
                },
                "aggs": {
                    "materials": {
                        "terms": {"field": "packing_info.material"}
                    }
                }
            }
        }
    }
    
    try:
        response = es.search(index=index_name, body=query)
        return response
    except Exception as e:
        print(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        return None

def print_ct_search_results(response, search_type: str):
    """CT ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥"""
    if not response:
        print(f"{search_type} ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
        return
    
    print(f"\n=== {search_type} ê²€ìƒ‰ ê²°ê³¼ ===")
    print(f"ì´ ê²€ìƒ‰ ê²°ê³¼: {response['hits']['total']['value']}ê°œ")
    
    for hit in response['hits']['hits']:
        source = hit['_source']
        score = hit['_score']
        print(f"\në¬¸ì„œ ID: {hit['_id']} (ì ìˆ˜: {score:.2f})")
        print(f"í…ŒìŠ¤íŠ¸ ë²ˆí˜¸: {source.get('test_no', 'N/A')}")
        print(f"ì œí’ˆëª…: {source.get('product_name', 'N/A')}")
        print(f"ê³ ê°ì‚¬: {source.get('customer', 'N/A')}")
        print(f"ê°œë°œì: {source.get('developer', 'N/A')}")
        print(f"í…ŒìŠ¤íŠ¸ ë‚ ì§œ: {source.get('test_date', 'N/A')}")
        print(f"ì‹¤í—˜ì‹¤ ID: {source.get('lab_id', 'N/A')}")
        
        # í•˜ì´ë¼ì´íŠ¸ ê²°ê³¼ ì¶œë ¥
        if 'highlight' in hit:
            print("ğŸ” í•˜ì´ë¼ì´íŠ¸ëœ ë§¤ì¹­ ë¶€ë¶„:")
            for field, highlights in hit['highlight'].items():
                print(f"  ğŸ“ {field}: {' ... '.join(highlights)}")
        
        print("-" * 80)

def print_ct_statistics(response):
    """CT ë¬¸ì„œ í†µê³„ ê²°ê³¼ ì¶œë ¥"""
    if not response:
        print("í†µê³„ ê²°ê³¼ ì—†ìŒ")
        return
    
    print(f"\n=== CT ë¬¸ì„œ í†µê³„ ===")
    
    aggs = response['aggregations']
    
    if 'customer_count' in aggs:
        print("\nê³ ê°ì‚¬ë³„ ë¬¸ì„œ ìˆ˜:")
        for bucket in aggs['customer_count']['buckets']:
            print(f"  {bucket['key']}: {bucket['doc_count']}ê°œ")
    
    if 'test_count_distribution' in aggs:
        print("\ní…ŒìŠ¤íŠ¸ ì°¨ìˆ˜ë³„ ë¶„í¬:")
        for bucket in aggs['test_count_distribution']['buckets']:
            print(f"  {bucket['key']}: {bucket['doc_count']}ê°œ")
    
    if 'date_distribution' in aggs:
        print("\nì›”ë³„ ë¬¸ì„œ ë¶„í¬:")
        for bucket in aggs['date_distribution']['buckets']:
            print(f"  {bucket['key_as_string']}: {bucket['doc_count']}ê°œ")

def load_json_files_from_directory(directory_path: str) -> List[Dict[str, Any]]:
    """ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ë“¤ì„ ë¡œë“œ"""
    import glob
    import os
    
    documents = []
    json_files = glob.glob(os.path.join(directory_path, "*.json"))
    
    print(f"ë°œê²¬ëœ JSON íŒŒì¼ ìˆ˜: {len(json_files)}")
    
    for file_path in json_files:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                documents.append(data)
                print(f"ë¡œë“œ ì™„ë£Œ: {os.path.basename(file_path)}")
        except Exception as e:
            print(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ {file_path}: {str(e)}")
    
    return documents

def load_and_index_ct_documents(index_name: str, directory_path: str):
    """JSON íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ê³  ì¸ë±ìŠ¤ì— ì‚½ì…"""
    print(f"ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ ë¡œë“œ ì¤‘: {directory_path}")
    documents = load_json_files_from_directory(directory_path)
    
    if not documents:
        print("ë¡œë“œí•  JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 0
    
    print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ì¸ë±ìŠ¤ì— ì‚½ì… ì¤‘...")
    success_count, error_count = bulk_insert_ct_documents(index_name, documents)
    
    return success_count, error_count

if __name__ == "__main__":
    index_name = "ct_documents"
    
    print("=== CT ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===\n")
    
    # 1. ì¸ë±ìŠ¤ ìƒì„±
    print("1. CT ë¬¸ì„œ ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    if create_ct_document_index_with_mapping(index_name):
        print("CT ë¬¸ì„œ ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ!")
        
        # 2. JSON íŒŒì¼ë“¤ ë¡œë“œ ë° ì¸ë±ì‹±
        print("\n2. JSON íŒŒì¼ ë¡œë“œ ë° ì¸ë±ì‹± ì¤‘...")
        refine_directory = "data/refine"
        success_count, error_count = load_and_index_ct_documents(index_name, refine_directory)
        
        if success_count > 0:
            print(f"\nì¸ë±ì‹± ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {error_count}ê°œ")
            
            # 3. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            print("\n3. ê²€ìƒ‰ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
            
            # ì œí’ˆëª… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            print("\n--- ì œí’ˆëª… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
            result = search_ct_documents_by_product_name(index_name, "Lip")
            print_ct_search_results(result, "Lip ì œí’ˆ ê²€ìƒ‰")
            
            # ê³ ê°ì‚¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            print("\n--- ê³ ê°ì‚¬ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
            result = search_ct_documents_by_customer(index_name, "Interstory")
            print_ct_search_results(result, "Interstory ê³ ê°ì‚¬ ê²€ìƒ‰")
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            print("\n--- ì „ì²´ í…ìŠ¤íŠ¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
            result = full_text_search_ct_documents(index_name, "íŒí•‘ í…ŒìŠ¤íŠ¸")
            print_ct_search_results(result, "íŒí•‘ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰")
            
            # ì¬ì§ˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            print("\n--- ì¬ì§ˆ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
            result = search_ct_documents_by_material(index_name, "AS")
            print_ct_search_results(result, "AS ì¬ì§ˆ ê²€ìƒ‰")
            
            # í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            print("\n--- í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
            result = search_ct_documents_by_test_code(index_name, "TMM202")
            print_ct_search_results(result, "TMM202 í…ŒìŠ¤íŠ¸ ì½”ë“œ ê²€ìƒ‰")
            
            # ê³ ê¸‰ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
            print("\n--- ê³ ê¸‰ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ---")
            search_params = {
                "product_name": "Lip",
                "customer": "Interstory",
                "search_text": "íŒí•‘"
            }
            result = advanced_search_ct_documents(index_name, search_params)
            print_ct_search_results(result, "ê³ ê¸‰ ê²€ìƒ‰ (Lip + Interstory + íŒí•‘)")
            
            # í†µê³„ ì¡°íšŒ í…ŒìŠ¤íŠ¸
            print("\n--- í†µê³„ ì¡°íšŒ í…ŒìŠ¤íŠ¸ ---")
            stats_result = get_ct_document_statistics(index_name)
            print_ct_statistics(stats_result)
            
        else:
            print("ì¸ë±ì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        print("CT ë¬¸ì„œ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨!")
